# Jukebox representation for MIR
__author__ = "dr.seunggoo.kim@gmail.com"
__refernece__ = [
"https://openai.com/blog/jukebox/","https://github.com/p-lambda/jukemir","https://arxiv.org/abs/2005.00341"
]

## Processing steps
1. ENCODING (VQ-VAE-encoder): waveform (44.1 kHz) to top-level (334 Hz)
2. SAMPLING (transformers): new top-level representation (334 Hz)
3. UPSAMPLING (transformers): from top-level (334 Hz) to bottom-level (5.5 kHz)
4. DECODING (VQ-VAE-decoder): bottom-level (5.5 kHz) to waveform (44.1 kHz)


## Components
1. VQ-VAE: encoding & decoding 3 levels (8x, 32x, 128x) cnn codebook size = 2048
2. TRANSFORMERS: sampling at the top-level & upsampling to bottom-level Three-level prior models using a simplified variant of Sparse Transformers (72 layers of factorized self-attention on a context of 8192 tokens).


## Hyperparameters
REF: https://arxiv.org/abs/2005.00341 (Tables 4-6)
"receptive fields" of residual blocks in middle/top VQ-VAEs = 120/480 ms/tkn" and 2 sec for the bottom-level? (pp. 6)
"8192 tokens of VQ-VAE codes correspond to 24/6/1.5 sec of raw audio at the top/middle/bottom level" (pp. 6)

### VQ-VAE hyperparameters
Sample rate = 44,100 [Hz]
Sample length = 393,216 [smp] (8.9165 [sec]?)
Codebook size = 2,048 [codes]
Bottom-level residual block: 2 [sec/tkn]
Hop lengths = 8, 32, 128 (T/S where T = #audioSamples, S=#tokens)
Time/token = 0.1814, 0.7256, 2.9024 [ms] for bottom/middle/top-level

### Bottom-level upsamplers
Context length = 8,192 [tokens] (# of tokens)
Hop (T/S) = 8 [smp/tkn]
Sample length = 65,536 [smp] (bottom; 1.4861 [sec])
Token length = 0.1814 [ms/tkn]
Transformer layers = 72 [layers]
Transformer width = 1,920 [dims]

### Middle-level upsamplers
Context length = 8,192 [tokens] (# of tokens)
Hop (T/S) = 32 [smp/tkn]
Sample length = 262,144 [smp] (middle; 5.9443 [sec])
Token length = 0.7256 [ms/tkn]
Transformer layers = 72 [layers]
Transformer width = 1,920 [dims]

### Top-level prior model: next-token prediction
Context length = 8,192 [tokens] (# of tokens)
Hop (T/S) = 128 [smp/tkn]
Sample length = 1,048,576 [smp] (5B; 23.7772 [sec]) | 786,432 [smp] (1B; 17.8329 [sec])
Token length = 2.9024 [ms/tkn]
Transformer self-attention layers = 72 [layers]
Transformer width = 4,800 [dims] (# of embedding dimensions)


